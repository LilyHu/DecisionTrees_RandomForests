{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "The stopping criteria for the decision trees are:\n",
    "- all the data at the current node belongs to one class\n",
    "\n",
    "or\n",
    "\n",
    "- the max depth of the tree has been reached\n",
    "\n",
    "Splits are on single features. For each feature, the mean of the means is used as the spliting value. The feature that minimizes entropy is selected. \n",
    "\n",
    "The relevant classes are:\n",
    "- DecisionTree\n",
    "- Node\n",
    "\n",
    "The most important functions are:\n",
    "- growTree, which recursively adds nodes, either children nodes or leaf nodes\n",
    "- segmentor, which determins the split rule\n",
    "- impurity, which calculates the impurity of a split\n",
    "\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "This random forest implementation has hyperparameters of the number of trees, the number of data points per tree, and the max depth of each tree. The random forest uses bootstrapping for each tree and bagging for each split. The decision trees in the random forest are modified from the standalone decision tree to call a modified segmentor. This segmentor is modified to select splits from a random subset of the features instead of across all features. \n",
    "\n",
    "The relevant classes are:\n",
    "- RandomForest, which contains a list of RandomDecisionTrees and bootstraps to grow the RandomDecisionTrees with different data\n",
    "- RandomDecisionTree (modified from DecisionTree to use growRandomizedTree)\n",
    "\n",
    "THe most important functions are:\n",
    "- growRandomizedTree (modifed from growTree to use randomizedSegmentor)\n",
    "- randomizedSegmentor (modified from segmentor to choose splits from a subset of features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Basic binary node\n",
    "'''\n",
    "class Node(object):\n",
    "    def __init__(self):\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.split_rule =[None, None] # this is a list. python doesn't have fixed type arrays!\n",
    "        self.label = None\n",
    "\n",
    "'''\n",
    "Standalone Decision Tree\n",
    "'''        \n",
    "class DecisionTree(object):\n",
    "    def __init__(self, max_tree_depth):\n",
    "        self.root = None\n",
    "        self.max_tree_depth = max_tree_depth\n",
    "    \n",
    "    def train(self, data, labels):\n",
    "        self.root = growTree(data, labels, self.max_tree_depth)\n",
    "            \n",
    "    def predict(self, test_data):\n",
    "        num_test_data = len(test_data)\n",
    "        predictions = []\n",
    "        path = []\n",
    "        for sample_i in xrange(0, num_test_data):\n",
    "            current_node = self.root\n",
    "            while current_node.label is None:\n",
    "                feature_to_split_on = current_node.split_rule[0]\n",
    "                feature_threshold = current_node.split_rule[1]\n",
    "                if test_data[sample_i, feature_to_split_on] < feature_threshold:\n",
    "                    current_node = current_node.left_child\n",
    "                else:\n",
    "                    current_node = current_node.right_child\n",
    "            predictions.append(current_node.label)        \n",
    "        return np.asarray(predictions)\n",
    "\n",
    "    def tracePredict(self, test_data):\n",
    "        #num_test_data = len(test_data)\n",
    "        predictions = []\n",
    "        path = []\n",
    "        \n",
    "        current_node = self.root\n",
    "        while current_node.label is None:\n",
    "            feature_to_split_on = current_node.split_rule[0]\n",
    "            feature_threshold = current_node.split_rule[1]\n",
    "            if test_data[feature_to_split_on] < feature_threshold:\n",
    "                current_node = current_node.left_child\n",
    "                path.append([current_node.split_rule, '<'])\n",
    "            else:\n",
    "                current_node = current_node.right_child\n",
    "                path.append([current_node.split_rule, '>'])\n",
    "        prediction = current_node.label        \n",
    "        return prediction, path\n",
    "\n",
    "'''\n",
    "Ramdom Forest\n",
    "'''\n",
    "class RandomForest(object):\n",
    "    def __init__(self, num_trees, max_tree_depth, num_samples_per_tree, num_possible_features_per_split):\n",
    "        self.trees = []\n",
    "        self.num_trees = num_trees\n",
    "        self.max_tree_depth = max_tree_depth\n",
    "        self.num_samples_per_tree = num_samples_per_tree\n",
    "        self.num_possible_features_per_split = num_possible_features_per_split\n",
    "        \n",
    "    def train(self, data, labels):\n",
    "        for tree_i in xrange(0, self.num_trees):\n",
    "            r_data, r_labels = randomizeDataMatrix(data, labels, self.num_samples_per_tree, True)\n",
    "            new_tree = RandomizedDecisionTree(self.max_tree_depth, self.num_possible_features_per_split)\n",
    "            new_tree.train(r_data, r_labels)\n",
    "            self.trees.append(new_tree)\n",
    "        \n",
    "    def predict(self, test_data):\n",
    "        votes = np.zeros((len(test_data), self.num_trees))\n",
    "        predictions = []\n",
    "        for tree_i in xrange(0, self.num_trees):\n",
    "            votes[:,tree_i] = self.trees[tree_i].predict(test_data)\n",
    "        for sample_i in xrange(0, len(test_data)):\n",
    "            prediction = getMode(votes[sample_i, :])\n",
    "            predictions.append(prediction)\n",
    "        return np.asarray(predictions)\n",
    "    \n",
    "'''\n",
    "RandomizedDecisionTree extends DecisionTree with a new self.root grown using growRandomizedTree\n",
    "instead of growTree\n",
    "'''\n",
    "class RandomizedDecisionTree(DecisionTree):\n",
    "    def __init__(self, max_tree_depth, num_possible_features_per_split):\n",
    "        self.root = None\n",
    "        self.max_tree_depth = max_tree_depth\n",
    "        self.num_possible_features_per_split = num_possible_features_per_split\n",
    "    \n",
    "    def train(self, data, labels):\n",
    "        self.root = growRandomizedTree(data, labels, self.max_tree_depth, self.num_possible_features_per_split)    \n",
    "\n",
    "'''\n",
    "MOdieifed from growTree to call randomizedSegmentor instead of segmentor\n",
    "'''        \n",
    "def growRandomizedTree(data, labels, max_tree_depth, num_possible_features_per_split):\n",
    "    node = Node()\n",
    "    split_rule, node_label = randomizedSegmentor(data, labels, num_possible_features_per_split)\n",
    "    \n",
    "    if max_tree_depth <= 0: # stop recursion\n",
    "        node.label = getMode(labels)\n",
    "    \n",
    "    elif split_rule == None:\n",
    "        node.label=node_label\n",
    "        # This node is a leaf node\n",
    "    else:\n",
    "        \n",
    "        data_left_child, labels_left_child, data_right_child, labels_right_child = divideDataForChildren(data, labels, split_rule)\n",
    "        if (len(data_left_child)==0) or ((len(data_right_child)==0)):\n",
    "            node.label=getMode(labels)\n",
    "        else:\n",
    "            node.split_rule = split_rule\n",
    "            node.right_child = growRandomizedTree(data_right_child, labels_right_child, max_tree_depth-1, num_possible_features_per_split)\n",
    "            node.left_child = growRandomizedTree(data_left_child, labels_left_child, max_tree_depth-1, num_possible_features_per_split)\n",
    "        \n",
    "    return node\n",
    "        \n",
    "'''\n",
    "Recurisvely adds nodes\n",
    "'''               \n",
    "def growTree(data, labels, max_tree_depth):\n",
    "    node = Node()\n",
    "    split_rule, node_label = segmentor(data, labels)\n",
    "    \n",
    "    if max_tree_depth <= 0: # stop recursion\n",
    "        node.label = getMode(labels)\n",
    "    \n",
    "    elif split_rule == None:\n",
    "        node.label=node_label\n",
    "        # This node is a leaf node\n",
    "    else:\n",
    "                \n",
    "        data_left_child, labels_left_child, data_right_child, labels_right_child = divideDataForChildren(data, labels, split_rule)\n",
    "        if (len(data_left_child)==0) or ((len(data_right_child)==0)):\n",
    "            node.label=getMode(labels)\n",
    "        else:    \n",
    "            node.split_rule = split_rule\n",
    "            data_left_child, labels_left_child, data_right_child, labels_right_child = divideDataForChildren(data, labels, split_rule)\n",
    "            node.right_child = growTree(data_right_child, labels_right_child, max_tree_depth-1)\n",
    "            node.left_child = growTree(data_left_child, labels_left_child, max_tree_depth-1)\n",
    "\n",
    "    return node\n",
    "\n",
    "'''\n",
    "Assign labels to a node using the mode of the labels\n",
    "'''\n",
    "def getMode(labels):\n",
    "    label_dict = {}\n",
    "    num_labels = len(labels)\n",
    "    for sample_i in xrange(0,num_labels):\n",
    "        if labels[sample_i] not in label_dict:\n",
    "            label_dict[labels[sample_i]] = 1\n",
    "        else:\n",
    "            label_dict[labels[sample_i]] += 1\n",
    "    \n",
    "    max_label = None\n",
    "    max_count = 0\n",
    "    for key_i in label_dict:\n",
    "        if label_dict[key_i] > max_count:\n",
    "            max_count = label_dict[key_i]\n",
    "            max_label = key_i\n",
    "    return max_label\n",
    "    \n",
    "    \n",
    "'''\n",
    "Creates data sets for the left and right children of a parent node after a split is determined\n",
    "Output: \n",
    "    data_left_child\n",
    "    labels_left_child\n",
    "    data_right_child\n",
    "    labels_right_child\n",
    "    \n",
    "Extension Notes\n",
    "    Binary split\n",
    "    Split on single feature\n",
    "'''\n",
    "def divideDataForChildren(data, labels, split_rule):\n",
    "    num_data = len(data)\n",
    "    split_rule_feature = split_rule[0]\n",
    "    split_rule_threshold = split_rule[1]\n",
    "    data_left_child = []\n",
    "    labels_left_child = []\n",
    "    data_right_child = []\n",
    "    labels_right_child = []\n",
    "    for sample_i in xrange(0, num_data):\n",
    "        if data[sample_i, split_rule_feature] < split_rule_threshold:\n",
    "            data_left_child.append(data[sample_i,:])\n",
    "            labels_left_child.append(labels[sample_i])\n",
    "        else:\n",
    "            data_right_child.append(data[sample_i, :])\n",
    "            labels_right_child.append(labels[sample_i])\n",
    "    return np.asarray(data_left_child), np.asarray(labels_left_child), np.asarray(data_right_child), np.asarray(labels_right_child)\n",
    "    \n",
    "'''\n",
    "Calcualtes the impurity due to a split\n",
    "Remember that the lower the impurity, the better\n",
    "Input:  \n",
    "    left_label_hist    2 element array-like\n",
    "    right_label_hist   2 element array-like\n",
    "\n",
    "Extension Notes:\n",
    "    Binary splits only\n",
    "        \n",
    "'''    \n",
    "def impurity(left_label_hist, right_label_hist):\n",
    "    total_children = float(sum(left_label_hist)+sum(right_label_hist))\n",
    "    impurity = sum(left_label_hist)/total_children*entropyOfNode(left_label_hist)+sum(right_label_hist)/total_children*entropyOfNode(right_label_hist)\n",
    "    return impurity\n",
    "\n",
    "'''\n",
    "Calculates the entropy of a node\n",
    "Input: \n",
    "    hist      an array-like object of size 2\n",
    "    \n",
    "Extension Notes:\n",
    "    Binary splits only\n",
    "'''\n",
    "def entropyOfNode(hist):\n",
    "    if (hist[0] == 0) or (hist[1] == 0):\n",
    "        entropy = 0\n",
    "    else: \n",
    "        total_children = float(sum(hist))\n",
    "        entropy = -(hist[0]/(total_children)*math.log(hist[0]/(total_children),2)+hist[1]/(total_children)*math.log(hist[1]/(total_children),2))\n",
    "    return entropy\n",
    "\n",
    "'''\n",
    "Returns best split and node_label, one of which will be None\n",
    "If all the data is sorted, returns the node_label\n",
    "\n",
    "Extension notes\n",
    "    What if split_rule = None and node_labe = None because all splits give impurity = 1???\n",
    "'''\n",
    "def segmentor(data, labels):\n",
    "    \n",
    "    num_features = shape(data)[1]\n",
    "    min_impurity = 1\n",
    "    split_rule = None\n",
    "    node_label = None\n",
    "    for feature_i in xrange(0,num_features):\n",
    "        node_split_feature_threshold, node_label_on_feature = splitSingleFeature(data[:,feature_i], labels)\n",
    "        if node_split_feature_threshold is None: # Check if all the elements are classified\n",
    "            split_rule = None\n",
    "            node_label = node_label_on_feature\n",
    "            return (split_rule, node_label)\n",
    "        else:\n",
    "            left_label_hist, right_label_hist = createLabelHist(data[:, feature_i], node_split_feature_threshold, labels)\n",
    "            #return left_label_hist, right_label_hist\n",
    "            impurity_of_feature = impurity(left_label_hist, right_label_hist)\n",
    "            # Check if this feature minimizes the impurity \n",
    "            if impurity_of_feature < min_impurity:\n",
    "                split_rule = [feature_i, node_split_feature_threshold]\n",
    "                node_label = None\n",
    "                min_impurity = impurity_of_feature\n",
    "    if (split_rule==None) and (node_label==None):\n",
    "        node_label = getMode(labels)\n",
    "    return (split_rule, node_label)\n",
    "\n",
    "'''\n",
    "Modified from segmentor to pick a split from a subset of features\n",
    "'''\n",
    "def randomizedSegmentor(data, labels, num_possible_features_per_split):\n",
    "    #print shape(data)\n",
    "    num_features = shape(data)[1]\n",
    "    min_impurity = 1\n",
    "    split_rule = None\n",
    "    node_label = None\n",
    "    possible_features = np.random.choice(range(0,num_features), num_possible_features_per_split)\n",
    "    \n",
    "    for feature_i in possible_features:\n",
    "        node_split_feature_threshold, node_label_on_feature = splitSingleFeature(data[:,feature_i], labels)\n",
    "        if node_split_feature_threshold is None: # Check if all the elements are classified\n",
    "            split_rule = None\n",
    "            node_label = node_label_on_feature\n",
    "            return (split_rule, node_label)\n",
    "        else:\n",
    "            left_label_hist, right_label_hist = createLabelHist(data[:, feature_i], node_split_feature_threshold, labels)\n",
    "            #return left_label_hist, right_label_hist\n",
    "            impurity_of_feature = impurity(left_label_hist, right_label_hist)\n",
    "            # Check if this feature minimizes the impurity \n",
    "            if impurity_of_feature < min_impurity:\n",
    "                split_rule = [feature_i, node_split_feature_threshold]\n",
    "                node_label = None\n",
    "                min_impurity = impurity_of_feature\n",
    "    if (split_rule==None) and (node_label==None):\n",
    "        node_label = getMode(labels)\n",
    "    return (split_rule, node_label)\n",
    "\n",
    "'''\n",
    "Calculates the split threshold on a single feature\n",
    "Input: \n",
    "    data is a vector of one feature\n",
    "'''    \n",
    "def splitSingleFeature(data, labels):\n",
    "    '''\n",
    "    IMPROVEMENT: A better way to do this is to arrange the points from smallest to largest and then\n",
    "    test each split as where the labels change\n",
    "    '''\n",
    "    # Split the data by label into a dict with labels as the keys\n",
    "    data_dict = {}\n",
    "    n = len(data)\n",
    "    for i in xrange(0, n):\n",
    "        if labels[i] in data_dict:\n",
    "            data_dict[labels[i]].append(data[i])\n",
    "        else:\n",
    "            data_dict[labels[i]]=[data[i]]\n",
    "        '''\n",
    "        if labels[i] not in data_dict:\n",
    "            data_dict[labels[i]] = []\n",
    "        data_dict[labels[i]].append(data[i])\n",
    "        '''\n",
    "    # Calculate the mean of the labels\n",
    "    mean_labels = {}\n",
    "    for label in data_dict: # this is python magic! It loops through the keys without writting data_dict.keys()\n",
    "        mean_labels[label] = np.mean(data_dict[label], axis=0)\n",
    "    # Check if a leaf node\n",
    "    if len(data_dict) == 1:\n",
    "        node_label = data_dict.keys()[0] # This is a leaf node\n",
    "        node_split_feature_threshold = None\n",
    "    else: \n",
    "        # Pick the threshold as the mean of means\n",
    "        # assums binary class split\n",
    "        node_split_feature_threshold = mean(mean_labels.values())\n",
    "        node_label = None\n",
    "        \n",
    "    return (node_split_feature_threshold, node_label)\n",
    "\n",
    "\n",
    "'''\n",
    "Creates a histogram of the data by label\n",
    "Input: \n",
    "    data                     vector\n",
    "    node_feature_threshold   threshold to split on\n",
    "    labels                   labels corresponding to the vector\n",
    "    \n",
    "Output: \n",
    "    left_label_dict.values()   histogram of the frequencies\n",
    "    right_label_dict.values()  histogram of the frequencies in the right child\n",
    "\n",
    "Extension notes:\n",
    "    Because the histograms are first implemented as dictionaries,\n",
    "    this function is ready to take on multiway splits\n",
    "\n",
    "'''\n",
    "def createLabelHist(data, node_split_feature_threshold, labels):\n",
    "    num_data = len(data)\n",
    "    left_label_dict = {}\n",
    "    right_label_dict = {}\n",
    "    \n",
    "    for data_i in xrange(0, num_data):\n",
    "        # Make sure the left_label_hist and right_label_hist have the same keys\n",
    "        if labels[data_i] not in left_label_dict:\n",
    "            left_label_dict[labels[data_i]] = 0\n",
    "            right_label_dict[labels[data_i]] = 0\n",
    "        # Enter the new data point into its corresponding histogram\n",
    "        if data[data_i] < node_split_feature_threshold:\n",
    "            left_label_dict[labels[data_i]] += 1\n",
    "        else:\n",
    "            right_label_dict[labels[data_i]] +=1\n",
    "    return left_label_dict.values(), right_label_dict.values()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

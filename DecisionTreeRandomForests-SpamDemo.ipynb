{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import DecisionTreeRandomForests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helpful data processing functions\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "Creates a new randomized data and labels\n",
    "Input:\n",
    "    replace_TF    True or False\n",
    "'''\n",
    "def randomizeDataMatrix(data, labels, num_samples, replace_TF):\n",
    "    new_indices = np.random.choice(range(0,len(data)), size=num_samples, replace=replace_TF)\n",
    "    new_data = np.zeros((num_samples, data.shape[1]))\n",
    "    new_labels = np.zeros(num_samples)\n",
    "    #print new_data.shape\n",
    "    for sample_i in xrange(0, num_samples):\n",
    "        new_data[sample_i, :] = data[new_indices[sample_i], :]\n",
    "        new_labels[sample_i] = labels[new_indices[sample_i]]\n",
    "    \n",
    "    return new_data, new_labels\n",
    "\n",
    "\n",
    "'''\n",
    "Takes in an array like object and returns True if there is a '?'\n",
    "'''\n",
    "def isThereQuestionMark(sample_values):\n",
    "    num_entries = len(sample_values)\n",
    "    for feature_i in xrange(0, num_entries):\n",
    "        if sample_values[feature_i] == '?':\n",
    "            #print 'True'\n",
    "            return True\n",
    "\n",
    "    #print 'False'\n",
    "    return False\n",
    "\n",
    "'''\n",
    "Calculates validation error\n",
    "'''\n",
    "def calculateValidationError(predictions, labels_valid):\n",
    "    return sum([a == b for a, b in zip(predictions, labels_valid)]) / float(len(predictions))\n",
    "\n",
    "\n",
    "'''\n",
    "Reads a csv file and then creates the data and labels array\n",
    "\n",
    "Getting a csv file into an array for decision trees is magical \n",
    "'''\n",
    "def readCSVToBinarizedArray(filepath):\n",
    "\n",
    "\n",
    "    # Read cvs into pandas dataframe\n",
    "    pd_of_csv = pandas.read_csv(filepath)\n",
    "    # Take each row of the dataframe and convert to a list\n",
    "    list_of_sample_data_dicts = []\n",
    "    list_of_sample_labels = []\n",
    "    num_samples = len(pd_of_csv)\n",
    "    for sample_i in xrange(0, num_samples):\n",
    "        dict_of_sample = pd_of_csv.loc[sample_i].to_dict()\n",
    "        if not isThereQuestionMark(dict_of_sample.values()): # Check there are no missing values, ignore those samples for now\n",
    "            # Create a list of dictionaries\n",
    "            try: \n",
    "                list_of_sample_labels.append(dict_of_sample.pop('label'))\n",
    "            except KeyError:\n",
    "                pass\n",
    "            list_of_sample_data_dicts.append(dict_of_sample)    \n",
    "\n",
    "    # Binarize\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    dict_vectorizer = DictVectorizer(sparse=False)    \n",
    "\n",
    "    # Create array of data\n",
    "    data_array = dict_vectorizer.fit_transform(list_of_sample_data_dicts)\n",
    "    #dict_vectorizer.inverse_transform(census_array)\n",
    "    return data_array, np.asarray(list_of_sample_labels), dict_vectorizer\n",
    "\n",
    "'''\n",
    "Convert CSV to binarized array using an existing dict vectorizer\n",
    "'''\n",
    "def readCSVToBinarizedArrayUsingTransform(filepath, dict_vectorizer):\n",
    "\n",
    "\n",
    "    # Read cvs into pandas dataframe\n",
    "    pd_of_csv = pandas.read_csv(filepath)\n",
    "    # Take each row of the dataframe and convert to a list\n",
    "    list_of_sample_data_dicts = []\n",
    "    num_samples = len(pd_of_csv)\n",
    "    for sample_i in xrange(0, num_samples):\n",
    "        dict_of_sample = pd_of_csv.loc[sample_i].to_dict()\n",
    "        '''\n",
    "        if not isThereQuestionMark(dict_of_sample.values()): # Check there are no missing values, ignore those samples for now\n",
    "            \n",
    "            # Create a list of dictionaries\n",
    "            try: \n",
    "                list_of_sample_labels.append(dict_of_sample.pop('label'))\n",
    "            except KeyError:\n",
    "                pass\n",
    "            '''\n",
    "        list_of_sample_data_dicts.append(dict_vectorizer.transform(dict_of_sample))    \n",
    "\n",
    "    #print num_samples\n",
    "    # Create array of data\n",
    "    #data_array = dict_vectorizer.transform(list_of_sample_dicts)\n",
    "    #dict_vectorizer.inverse_transform(census_array)\n",
    "    return np.squeeze(np.asarray(list_of_sample_data_dicts))\n",
    "\n",
    "'''\n",
    "Split data into training and validation sets\n",
    "'''\n",
    "def splitTrainValidation(data, labels, num_validation_points):\n",
    "    train_data = data[0:len(data)-num_validation_points, :]\n",
    "    train_labels = labels[0:len(labels)-num_validation_points]\n",
    "    validation_data = data[len(data)-num_validation_points:, :]\n",
    "    validation_labels = labels[len(data)-num_validation_points:]\n",
    "    return train_data, train_labels, validation_data, validation_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SPAM\n",
    "\n",
    "'''\n",
    "\n",
    "# Load data\n",
    "spam_data = scipy.io.loadmat(\"spam_data\")\n",
    "data_train = spam_data['training_data']\n",
    "labels_train = spam_data['training_labels']\n",
    "labels_train = labels_train.transpose().flatten()\n",
    "r_data, r_labels = randomizeDataMatrix(data_train, labels_train, len(data_train), False)\n",
    "data_train = r_data[0:4000]\n",
    "labels_train = r_labels[0:4000]\n",
    "data_validation = r_data[4000:]\n",
    "labels_validation = r_labels[4000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy for the decision tree is: 0.84130\n"
     ]
    }
   ],
   "source": [
    "# Create decision tree\n",
    "spamDT = DecisionTreeRandomForests.DecisionTree(25)\n",
    "spamDT.train(data_train, labels_train)\n",
    "predictions = spamDT.predict(data_validation)\n",
    "error = calculateValidationError(predictions, labels_validation)\n",
    "print 'The validation accuracy for the decision tree is: %.5f' %error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy for the random forest is: 0.83959\n"
     ]
    }
   ],
   "source": [
    "# Create random forest\n",
    "# (num_trees, max_tree_depth, num_samples_per_tree, num_possible_features_per_split)\n",
    "spamRF = DecisionTreeRandomForests.RandomForest(99, 25, 1000, 20)\n",
    "spamRF.train(data_train, labels_train)\n",
    "predictions = spamRF.predict(data_validation)\n",
    "error = calculateValidationError(predictions, labels_validation)\n",
    "print 'The validation accuracy for the random forest is: %.5f' %error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sample #25, which is classified as ham, the splits are:\n",
      "Split # 1 is ' meter ' < 0.323457294647\n",
      "Split # 2 is ' ( ' < 0.776984139949\n",
      "Split # 3 is ' volumnes ' < 0.0569105691057\n",
      "Split # 4 is ' & ' < 0.141981064114\n",
      "Split # 5 is ' perscription ' < 0.0216919739696\n",
      "Split # 6 is ' ; ' < 0.301921266538\n",
      "Split # 7 is ' pain ' < 0.0198126104283\n",
      "Split # 8 is ' [ ' < 0.0268306092867\n",
      "Split # 9 is ' memo ' < 0.0115176151762\n",
      "Split # 10 is ' path ' < 0.0158620689655\n",
      "Split # 11 is ' bank ' < 0.0276243391952\n",
      "Split # 12 is ' energy ' < 0.0533799542348\n",
      "Split # 13 is ' drug ' < 0.00531914893617\n",
      "Split # 14 is ' differ ' < 0.00672043010753\n",
      "Split # 15 is ' spam ' < 0.00679347826087\n",
      "Split # 16 is ' message ' < 0.0547904556725\n",
      "Split # 17 is ' private ' < 0.0124976596143\n",
      "Split # 18 is ' planning ' < 0.00460829493088\n",
      "Split # 19 is ' business ' < 0.0355620155039\n",
      "Split # 20 is ' revision ' < 0.00314465408805\n",
      "Split # 21 is ' height ' < 0.00153374233129\n",
      "Split # 22 is ' featured ' < 0.00153846153846\n",
      "Split # 23 is ' pleased ' < 0.00157977883096\n",
      "Split # 24 is ' recrod ' < 0.00621441568351\n"
     ]
    }
   ],
   "source": [
    "# Trace the path of a sample point\n",
    "prediction, path = spamDT.tracePredict(r_data[25])\n",
    "features = ['pain', 'private', 'bank', 'money', 'drug', 'spam', 'perscription', 'creative', 'height', 'featured', 'differ', 'width', 'other', 'energy', 'business', 'message', 'volumnes', 'revision', 'path', 'meter', 'memo', 'planning', 'pleased', 'recrod', 'out', ';', '$', '#', '!', '(', '[','&']\n",
    "print \"For sample #25, which is classified as ham, the splits are:\"\n",
    "for i in xrange(0, len(path)-1):\n",
    "    print \"Split #\", i+1, \"is '\",features[path[i][0][0]],\"'\", path[i][1], path[i][0][1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the random forest, the most common splits made at the root nodes of the trees are:\n",
      "' ! ' < approx 0.955805173397 ( 37 trees )\n",
      "' meter ' < approx 0.331808474386 ( 30 trees )\n",
      "' volumnes ' < approx 0.112926855822 ( 13 trees )\n",
      "' & ' < approx 0.21250299418 ( 10 trees )\n",
      "' money ' < approx 0.110665516855 ( 4 trees )\n",
      "' perscription ' < approx 0.0476169851967 ( 3 trees )\n",
      "' ( ' < approx 0.903805719595 ( 1 trees )\n",
      "' featured ' < approx 0.0213114754098 ( 1 trees )\n"
     ]
    }
   ],
   "source": [
    "# Find the root split of all trees\n",
    "all_tree_splits = []\n",
    "split_freq = np.zeros(32)\n",
    "for i in xrange(0, len(spamRF.trees)):\n",
    "    split = spamRF.trees[i].root.split_rule\n",
    "# Get the data per side of the tree to determine whether < or >\n",
    "    left_data, left_labels, right_data, right_labels = divideDataForChildren(r_data, r_labels, split)\n",
    "    if len(left_data) > len(right_data):\n",
    "        all_tree_splits.append([split, 0])\n",
    "    else:\n",
    "        all_tree_splits.append([split, 1])\n",
    "\n",
    "# Count the frequency of splitting on the features and the threshold values\n",
    "common_split = {}\n",
    "split_freq = np.zeros(32)\n",
    "for i in xrange(0, len(spamRF.trees)):\n",
    "    split = all_tree_splits[i][0]\n",
    "    if split[0] not in common_split:\n",
    "        common_split[split[0]]=[]\n",
    "    common_split[split[0]].append(split[1])\n",
    "    split_freq[split[0]] += 1\n",
    "    \n",
    "# Sort by most common feature\n",
    "split_freq_dict = dict(zip(np.arange(0, 100), split_freq))\n",
    "\n",
    "import operator\n",
    "#x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n",
    "#sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "\n",
    "sorted_common_split = sorted(split_freq_dict.items(), key=operator.itemgetter(1))\n",
    "\n",
    "# Print the most common splits\n",
    "print \"For the random forest, the most common splits made at the root nodes of the trees are:\"\n",
    "for i in xrange(0, len(features)):\n",
    "    if sorted_common_split[-1-i][1] > 0:\n",
    "        feature = sorted_common_split[-1-i][0]\n",
    "        threshold = np.mean(common_split[sorted_common_split[-1-i][0]])\n",
    "        num_trees = sorted_common_split[-1-i][1]\n",
    "        print \"'\",features[feature],\"'\", '< approx', threshold, \"(\", int(num_trees), \"trees )\"\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "## a) Decision Tree\n",
    "\n",
    "The stopping criteria for the decision trees are:\n",
    "- all the data at the current node belongs to one class\n",
    "\n",
    "or\n",
    "\n",
    "- the max depth of the tree has been reached\n",
    "\n",
    "Splits are on single features. For each feature, the mean of the means is used as the spliting value. The feature that minimizes entropy is selected. \n",
    "\n",
    "The relevant classes are:\n",
    "- DecisionTree\n",
    "- Node\n",
    "\n",
    "The most important functions are:\n",
    "- growTree, which recursively adds nodes, either children nodes or leaf nodes\n",
    "- segmentor, which determins the split rule\n",
    "- impurity, which calculates the impurity of a split\n",
    "\n",
    "\n",
    "## b) Random Forest\n",
    "\n",
    "This random forest implementation has hyperparameters of the number of trees, the number of data points per tree, and the max depth of each tree. The random forest uses bootstrapping for each tree and bagging for each split. The decision trees in the random forest are modified from the standalone decision tree to call a modified segmentor. This segmentor is modified to select splits from a random subset of the features instead of across all features. \n",
    "\n",
    "The relevant classes are:\n",
    "- RandomForest, which contains a list of RandomDecisionTrees and bootstraps to grow the RandomDecisionTrees with different data\n",
    "- RandomDecisionTree (modified from DecisionTree to use growRandomizedTree)\n",
    "\n",
    "THe most important functions are:\n",
    "- growRandomizedTree (modifed from growTree to use randomizedSegmentor)\n",
    "- randomizedSegmentor (modified from segmentor to choose splits from a subset of features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
